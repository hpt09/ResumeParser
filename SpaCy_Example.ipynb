{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpaCy Example",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNb9zcelCSFG5/3tLf+Urey",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hpt09/ResumeParser/blob/master/SpaCy_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LaKQ99ytxzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "958dd6ce-b25e-4ec8-c1e5-c9e2100f53aa"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.2.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEvKym-BuCKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# Import required modules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-S3lBh4uDbf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "486ab9a5-7dcf-4dec-d255-faab8be4a8c5"
      },
      "source": [
        "test = spacy.load('en')\n",
        "sent = '''My name is Amar Sharma, i stay in Mumbai.\n",
        "The 2020 america presidential election is scheduled for Tuesday, November 3.'''\n",
        "\n",
        "ts = test(sent)\n",
        "for ent in ts.ents:\n",
        "  print(f'{ent.label_.upper():{10}} - {ent.text}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PERSON     - Amar Sharma\n",
            "GPE        - Mumbai\n",
            "DATE       - 2020\n",
            "GPE        - america\n",
            "DATE       - Tuesday, November 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9Bm5bZxvxGw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ac70ea5b-1915-4492-c1e1-0eb42e6b289f"
      },
      "source": [
        "for ent in test(\"Apple is looking at buying U.K. startup for $1 billion\").ents:\n",
        "  print(f'{ent.label_.upper():{10}} - {ent.text}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ORG        - Apple\n",
            "GPE        - U.K.\n",
            "MONEY      - $1 billion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zs9fqPlyhOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5b0194d8-294c-4628-a0f6-8158db64a76b"
      },
      "source": [
        "pip install PyMuPDF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyMuPDF\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/a1/11e63fd3746e0e6e07e71226cb51ef8a3cc2a2ad5577be04e8470035ce4a/PyMuPDF-1.17.5-cp36-cp36m-manylinux2010_x86_64.whl (6.0MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.17.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqX4Fc99yrJ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "f10dd346-d617-42b1-b31d-760493b31426"
      },
      "source": [
        "import sys,fitz\n",
        "doc= fitz.open('Alice Clark CV.pdf')\n",
        "alice_cv=\"\"\n",
        "for page in doc:\n",
        "  alice_cv = alice_cv + str(page.getText())\n",
        "\n",
        "print(alice_cv)\n",
        "\n",
        "# we have extracted the data from pdf file using PyMuPDF and stored in alice_cv variable.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alice Clark \n",
            "AI / Machine Learning \n",
            " \n",
            "Delhi, India Email me on Indeed \n",
            "• \n",
            "20+ years of experience in data handling, design, and development \n",
            "• \n",
            "Data Warehouse: Data analysis, star/snow flake scema data modelling and design specific to \n",
            "data warehousing and business intelligence \n",
            "• \n",
            "Database: Experience in database designing, scalability, back-up and recovery, writing and \n",
            "optimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes. \n",
            "Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, \n",
            "Stream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake \n",
            "analytics(U-SQL) \n",
            "Willing to relocate anywhere \n",
            " \n",
            "WORK EXPERIENCE \n",
            "Software Engineer \n",
            "Microsoft – Bangalore, Karnataka \n",
            "January 2000 to Present \n",
            "1. Microsoft Rewards Live dashboards: \n",
            "Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping \n",
            "online. Microsoft Rewards members can earn points when searching with Bing, browsing with \n",
            "Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft \n",
            "Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft \n",
            "rewards website. Rewards live dashboards gives a live picture of usage world-wide and by \n",
            "markets like US, Canada, Australia, new user registration count, top/bottom performing rewards \n",
            "offers, orders stats and weekly trends of user activities, orders and new user registrations. the \n",
            "PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes. \n",
            "Technology/Tools used \n",
            " \n",
            "EDUCATION \n",
            "Indian Institute of Technology – Mumbai \n",
            "2001 \n",
            " \n",
            "SKILLS \n",
            "Machine Learning, Natural Language Processing, and Big Data Handling \n",
            " \n",
            "ADDITIONAL INFORMATION \n",
            "Professional Skills \n",
            "• Excellent analytical, problem solving, communication, knowledge transfer and interpersonal \n",
            "skills with ability to interact with individuals at all the levels \n",
            "• Quick learner and maintains cordial relationship with project manager and team members and \n",
            "good performer both in team and independent job environments \n",
            "• Positive attitude towards superiors &amp; peers \n",
            "• Supervised junior developers throughout project lifecycle and provided technical assistance \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYl3zOnizw4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = spacy.load('en')\n",
        "ts = test(\" \".join(alice_cv.split('\\n'))) # we have splitted our data with '\\n' and rejoined with space. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBSR384azzYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ae9e914d-7142-4870-c121-d3d21c86440f"
      },
      "source": [
        "print(ts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alice Clark  AI / Machine Learning    Delhi, India Email me on Indeed  •  20+ years of experience in data handling, design, and development  •  Data Warehouse: Data analysis, star/snow flake scema data modelling and design specific to  data warehousing and business intelligence  •  Database: Experience in database designing, scalability, back-up and recovery, writing and  optimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes.  Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure,  Stream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake  analytics(U-SQL)  Willing to relocate anywhere    WORK EXPERIENCE  Software Engineer  Microsoft – Bangalore, Karnataka  January 2000 to Present  1. Microsoft Rewards Live dashboards:  Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping  online. Microsoft Rewards members can earn points when searching with Bing, browsing with  Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft  Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft  rewards website. Rewards live dashboards gives a live picture of usage world-wide and by  markets like US, Canada, Australia, new user registration count, top/bottom performing rewards  offers, orders stats and weekly trends of user activities, orders and new user registrations. the  PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes.  Technology/Tools used    EDUCATION  Indian Institute of Technology – Mumbai  2001    SKILLS  Machine Learning, Natural Language Processing, and Big Data Handling    ADDITIONAL INFORMATION  Professional Skills  • Excellent analytical, problem solving, communication, knowledge transfer and interpersonal  skills with ability to interact with individuals at all the levels  • Quick learner and maintains cordial relationship with project manager and team members and  good performer both in team and independent job environments  • Positive attitude towards superiors &amp; peers  • Supervised junior developers throughout project lifecycle and provided technical assistance  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxv1dv-Mz-QO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7887203b-05a2-4d09-ccc2-ac3a05561528"
      },
      "source": [
        "for ent in ts.ents:\n",
        "  if ent.label_.upper() == 'PERSON':\n",
        "    print(f'{ent.label_.upper():{10}} - {ent.text}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PERSON     - Alice Clark\n",
            "PERSON     - Stored Procedures\n",
            "PERSON     - Cloud\n",
            "PERSON     - Document DB\n",
            "PERSON     - Web Job\n",
            "PERSON     - Web App\n",
            "PERSON     - Bing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddPI1DBI0HRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a54b769b-ca49-4564-bc59-d46bcfca7252"
      },
      "source": [
        "# we have extracted the data from pdf file using PyMuPDF and stored in alice_cv variable.\n",
        "\n",
        "for ent in ts.ents:\n",
        "  if ent.label_.upper() == 'ORG':\n",
        "    print(f'{ent.label_.upper():{10}} - {ent.text}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ORG        - AI / Machine Learning\n",
            "ORG        - star/snow flake scema\n",
            "ORG        - SQL\n",
            "ORG        - Microsoft Azure\n",
            "ORG        - SQL Azure\n",
            "ORG        - Stream Analytics\n",
            "ORG        - Power BI\n",
            "ORG        - Power BI\n",
            "ORG        - SQL\n",
            "ORG        - Microsoft\n",
            "ORG        - Microsoft\n",
            "ORG        - Microsoft\n",
            "ORG        - Microsoft\n",
            "ORG        - Microsoft Edge\n",
            "ORG        - Microsoft\n",
            "ORG        - Microsoft\n",
            "ORG        - PBI\n",
            "ORG        - Technology/Tools\n",
            "ORG        - Indian Institute of Technology\n",
            "ORG        - Big Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seKlagmO0RK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d0cc387-1bc1-408a-ba32-b668c4f3f7d6"
      },
      "source": [
        "train_data = pickle.load(open('train_data.pkl','rb'))\n",
        "print(f\"Training data consist of {len(train_data)} manually labelled resume's.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data consist of 200 manually labelled resume's.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl90-vNy1qLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "8ef888c7-ffe5-4350-9992-536c890c6190"
      },
      "source": [
        "# Checking format of one resume data\n",
        "\n",
        "train_data[97]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Ramesh chokkala Telangana - Email me on Indeed: indeed.com/r/Ramesh-chokkala/16d5fa56f8c19eb6  WORK EXPERIENCE  software  Microsoft,Infosis, Google -  May 2018 to Present  software  Microsoft,Infosis, Google -  May 2018 to Present  EDUCATION  btech  Trinity engineering college  https://www.indeed.com/r/Ramesh-chokkala/16d5fa56f8c19eb6?isid=rex-download&ikw=download-top&co=IN',\n",
              " {'entities': [(250, 278, 'College Name'),\n",
              "   (243, 248, 'Degree'),\n",
              "   (182, 207, 'Companies worked at'),\n",
              "   (172, 180, 'Designation'),\n",
              "   (122, 147, 'Companies worked at'),\n",
              "   (112, 120, 'Designation'),\n",
              "   (48, 94, 'Email Address'),\n",
              "   (16, 25, 'Location'),\n",
              "   (0, 15, 'Name')]})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx8ecPb128Qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading blank spacy model as we want to customize our model.\n",
        "# spacy.blank('en') will create a blank model of a given language class i.e., for here English.\n",
        "\n",
        "nlp = spacy.blank('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MqtRXFK3bCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a function to train our model\n",
        "\n",
        "def train_model(train_data):\n",
        "    \n",
        "  if 'ner' not in nlp.pipe_names:# Checking if NER is present in pipeline\n",
        "    ner = nlp.create_pipe('ner') # creating NER pipe if not present\n",
        "    nlp.add_pipe(ner, last=True) # adding NER pipe in the end\n",
        "\n",
        "  for _, annotation in train_data: # Getting 1 resume at a time from our training data of 200 resumes\n",
        "    for ent in annotation['entities']: # Getting each tuple at a time from 'entities' key in dictionary at index[1] i.e.,(0, 15, 'Name') and so on\n",
        "      ner.add_label(ent[2])  # here we are adding only labels of each tuple from entities key dict, eg:- 'Name' label of (0, 15, 'Name')\n",
        "    \n",
        "  # In above for loop we finally added all custom NER from training data.\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] # getting all other pipes except NER.\n",
        "  with nlp.disable_pipes(*other_pipes): # Disabling other pipe's as we want to train only NER.\n",
        "        optimizer = nlp.begin_training()\n",
        "        \n",
        "        for itn in range(10):         # trainig model for 10 iteraion\n",
        "            print('Starting iteration ' + str(itn))\n",
        "            random.shuffle(train_data) # shuffling data in every iteration \n",
        "            losses = {}\n",
        "            for text, annotations in train_data:\n",
        "              try:\n",
        "                nlp.update(\n",
        "                    [text],        #batch of texts\n",
        "                    [annotations], #batch of annotations\n",
        "                    drop=0.2,      #dropout rate -makes it harder to memorise\n",
        "                    sgd=optimizer, #callable to update weights\n",
        "                    losses=losses) #Dictionary to update with the loss, keyed by pipeline component.\n",
        "              except Exception as e:\n",
        "                pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCPm2B-E34EW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "97e1ec3c-2b0e-4fa1-ea11-45821dd4d2e4"
      },
      "source": [
        "train_model(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting iteration 0\n",
            "Starting iteration 1\n",
            "Starting iteration 2\n",
            "Starting iteration 3\n",
            "Starting iteration 4\n",
            "Starting iteration 5\n",
            "Starting iteration 6\n",
            "Starting iteration 7\n",
            "Starting iteration 8\n",
            "Starting iteration 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gBgTxJh4UV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving our trained model to re-use.\n",
        "\n",
        "nlp.to_disk('nlp_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYemk7Hn4YK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading our trained model\n",
        "\n",
        "nlp_model = spacy.load('nlp_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LMrvG_04bgW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2bcdff26-af13-454e-ec86-78c3c2370b3d"
      },
      "source": [
        "# Checking all the custom NER created\n",
        "\n",
        "nlp_model.entity.labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('College Name',\n",
              " 'Companies worked at',\n",
              " 'Degree',\n",
              " 'Designation',\n",
              " 'Email Address',\n",
              " 'Graduation Year',\n",
              " 'Location',\n",
              " 'Name',\n",
              " 'Skills',\n",
              " 'UNKNOWN',\n",
              " 'Years of Experience')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2oevWpk4l42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "e67002b8-7f57-4415-d18f-8c85879ba48a"
      },
      "source": [
        "doc = nlp_model(\" \".join(alice_cv.split('\\n')))\n",
        "for ent in doc.ents:\n",
        "  print(f'{ent.label_.upper():{20}} - {ent.text}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NAME                 - Alice Clark\n",
            "LOCATION             - Delhi\n",
            "DEGREE               - 20+ years of experience in data handling, design, and development\n",
            "COMPANIES WORKED AT  - Microsoft\n",
            "DESIGNATION          - Software Engineer\n",
            "COMPANIES WORKED AT  - Microsoft\n",
            "COMPANIES WORKED AT  - Microsoft\n",
            "COMPANIES WORKED AT  - Microsoft\n",
            "COMPANIES WORKED AT  - Microsoft\n",
            "COMPANIES WORKED AT  - Microsoft\n",
            "COMPANIES WORKED AT  - Microsoft\n",
            "COMPANIES WORKED AT  - Microsoft\n",
            "DEGREE               - Indian Institute of Technology\n",
            "SKILLS               - Machine Learning, Natural Language Processing, and Big Data Handling    ADDITIONAL INFORMATION  Professional Skills  • Excellent analytical, problem solving, communication, knowledge transfer and interpersonal  skills with ability to interact with individuals at all the levels  • Quick learner and maintains cordial relationship with project manager and team members and  good performer both in team and independent job environments  • Positive attitude towards superiors &amp; peers  • Supervised junior developers throughout project lifecycle and provided technical assistance\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}